{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "binary models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "YPxCYOUdIn0Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "c2b6b985-6229-4877-a6ba-6a46b04fd09f"
      },
      "cell_type": "code",
      "source": [
        "!pip install keras==1.2.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==1.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/3e/9926ce5c678b7a7978724a2ecf24857d89a415d152b8d3443e6d45c228b2/Keras-1.2.2.tar.gz (175kB)\n",
            "\r\u001b[K    5% |█▉                              | 10kB 17.9MB/s eta 0:00:01\r\u001b[K    11% |███▊                            | 20kB 2.8MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 30kB 3.4MB/s eta 0:00:01\r\u001b[K    23% |███████▌                        | 40kB 3.0MB/s eta 0:00:01\r\u001b[K    29% |█████████▍                      | 51kB 3.3MB/s eta 0:00:01\r\u001b[K    35% |███████████▏                    | 61kB 3.9MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 71kB 4.2MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 81kB 4.0MB/s eta 0:00:01\r\u001b[K    52% |████████████████▉               | 92kB 4.4MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▊             | 102kB 4.6MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▌           | 112kB 4.6MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▍         | 122kB 5.7MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 133kB 5.5MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 143kB 7.1MB/s eta 0:00:01\r\u001b[K    87% |████████████████████████████    | 153kB 7.3MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 163kB 6.6MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▊| 174kB 7.1MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 184kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.0.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from theano->keras==1.2.2) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from theano->keras==1.2.2) (1.1.0)\n",
            "Building wheels for collected packages: keras\n",
            "  Running setup.py bdist_wheel for keras ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/55/07/cf/b32db0a8d243b2fd6759d5d7cb650aa20670b2b740209cbf7e\n",
            "Successfully built keras\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.4\n",
            "    Uninstalling Keras-2.2.4:\n",
            "      Successfully uninstalled Keras-2.2.4\n",
            "Successfully installed keras-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qpX0tBaF3E3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "outputId": "d84a2611-c5c9-4d26-ccf0-2499a895e00b"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-080648b6e71b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mvcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "YNwgm6FJ3Gii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e624e9cf-9c1c-4ab8-f0e8-f100a82c007f"
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fuse: mountpoint is not empty\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NKuNwOve4LhQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "import functools\n",
        "from skimage import io\n",
        "from skimage.segmentation import quickshift\n",
        "import glob\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2 \n",
        "import numpy as np\n",
        "from skimage import color"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3jOdZhJi5zIs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib.collections import PatchCollection\n",
        "import cv2 as cv\n",
        "from skimage import io\n",
        "from skimage.segmentation import quickshift\n",
        "import glob\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KCbOg5FK3Gns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "83dcfab4-a9d1-4ff1-b249-5f3d98c50526"
      },
      "cell_type": "code",
      "source": [
        "img_dir='drive/inter_iit/conv_data/sat_match2'\n",
        "label_dir='drive/inter_iit/conv_data/gt_binary_masks/dark_green'\n",
        "\n",
        "x_train_filenames = []\n",
        "y_train_filenames = []\n",
        "for index in range( 14):\n",
        "  \n",
        "  x_train_filenames.append(os.path.join(img_dir, \"{}.png\".format(index)))\n",
        "  y_train_filenames.append(os.path.join(label_dir, \"{}.png\".format(index)))\n",
        "  #print(index)\n",
        "\n",
        "print(len(x_train_filenames))  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i-SzVCpA3GvG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "9bfe68df-e1ef-43a1-9aba-7a56f22d3abc"
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "sz=(1024,1024)\n",
        "images_gt=[]\n",
        "images_sat=[]\n",
        "\n",
        "for i in range(0,14):\n",
        "  img_num=i\n",
        "  x_pathname = x_train_filenames[img_num]\n",
        "  y_pathname = y_train_filenames[img_num]\n",
        "  \n",
        " \n",
        "\n",
        "  a=((io.imread(x_pathname)))[...,0:3]\n",
        "  #a=cv2.resize(a,sz)\n",
        "  images_sat.append(a)\n",
        "  \n",
        "  \n",
        "  \n",
        "  b= color.rgb2gray(io.imread(y_pathname)[...,0:3])\n",
        "  #b=cv2.resize(b,sz)\n",
        "  images_gt.append(b)\n",
        "  print(i)\n",
        "  \n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2z9_v-A23Gyh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "images_gt=np.asarray(images_gt)\n",
        "images_sat=np.asarray(images_sat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aho-v0G2Rdsl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "25ff68c4-6ded-40d6-ee70-97c60841d1c0"
      },
      "cell_type": "code",
      "source": [
        "print(np.unique(images_sat[2]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KcF3m5is4fFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "7929fb78-fb86-42c0-d0b3-d7c2c323fbd5"
      },
      "cell_type": "code",
      "source": [
        " \n",
        "h,w=256,256\n",
        "\n",
        "split_sat=[]\n",
        "split_gt=[]\n",
        "t=0\n",
        "for i in range(0, 14):\n",
        "  \n",
        "  img_num=i\n",
        "  x,y=0,0\n",
        "  x_pathname = x_train_filenames[img_num]\n",
        "  y_pathname = y_train_filenames[img_num]\n",
        "  im_sat=images_sat[i]\n",
        "  im_gt=images_gt[i]\n",
        "  n=i\n",
        "  print(i)\n",
        "  \n",
        "  while 1:\n",
        "    x=0\n",
        "    while 1:\n",
        "      \n",
        "      fname_lab=os.path.join(label_dir, \"{}_{}_{}.png\".format(n,x,y))\n",
        "      fname_img=os.path.join(img_dir, \"{}_{}_{}.png\".format(n,x,y))\n",
        "      a,b,c=im_sat[y:y+h, x:x+w].shape\n",
        "      c,d=im_gt[y:y+h, x:x+w].shape\n",
        "      #print(c)\n",
        "      #print(d)\n",
        "      \n",
        "      \n",
        "      if(a==256 and b==256 and c==256 and d==256):\n",
        "        \n",
        "        t=t+1\n",
        "        \n",
        "        #print(t)\n",
        "        if(t==1):\n",
        "          crop_img_x = im_sat[y:y+h, x:x+w]#.flatten()\n",
        "          a=crop_img_x.reshape(1,h*w*3)\n",
        "          #df_sat=pd.DataFrame(a)\n",
        "          crop_img_y = im_gt[y:y+h, x:x+w]\n",
        "          #b=crop_img_y.reshape(1,h*w*1)\n",
        "          split_sat.append(crop_img_x)\n",
        "          split_gt.append(crop_img_y)\n",
        "          #df_gt=pd.DataFrame(b)\n",
        "          #print(\"dddddddddd\")\n",
        "        else:\n",
        "          crop_img_x = im_sat[y:y+h, x:x+w]#.flatten()\n",
        "          a=crop_img_x.reshape(1,h*w*3)\n",
        "          #temp=pd.DataFrame(a)\n",
        "          #df_sat = pd.concat([df_sat, temp], axis=0, ignore_index=True)\n",
        "          #plt.imsave(fname_img,crop_img_x)\n",
        "          #print(crop_img_y.shape)\n",
        "          split_sat.append(crop_img_x)\n",
        "\n",
        "          crop_img_y = im_gt[y:y+h, x:x+w]#.flatten()\n",
        "          #b=crop_img_y.reshape(1,h*w*1)\n",
        "          #temp2=pd.DataFrame(b)\n",
        "          #df_gt = pd.concat([df_gt, temp2], axis=0, ignore_index=True)\n",
        "          #plt.imsave(fname_lab,crop_img_y)\n",
        "          split_gt.append(crop_img_y)\n",
        "      x=x+40\n",
        "     # print(x)\n",
        "      if(x>=950):\n",
        "        break\n",
        "    y=y+40\n",
        "    if(y>=950):\n",
        "      break\n",
        "      \n",
        "    \n",
        "  \n",
        " "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EEXfP9rVcKoT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "split_gt=np.asarray(split_gt)\n",
        "split_sat=np.asarray(split_sat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_NETxzjvzQ4C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5f1b609b-8b5d-4444-afab-4fe507a6eb56"
      },
      "cell_type": "code",
      "source": [
        "print(split_gt[1].shape)\n",
        "a=split_gt[1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qeu-5s_03HBG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "img_shape = (256, 256, 3)\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GvFa65Rb3VlH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test= train_test_split(split_sat, split_gt, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FJ81ZxoDiQfb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "im_shape = (256, 256, 3)\n",
        "im_shape_label=(256,256,1)\n",
        "X_train = X_train.reshape(X_train.shape[0], *im_shape)\n",
        "y_train = y_train.reshape(y_train.shape[0], *im_shape_label)\n",
        "X_test = X_test.reshape(X_test.shape[0], *im_shape)\n",
        "y_test = y_test.reshape(y_test.shape[0], *im_shape_label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vfsKB_Cgb_5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3e868f58-a60c-40be-a207-7211bd55b56a"
      },
      "cell_type": "code",
      "source": [
        "print((y_train.shape[0]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bDu4edNFYBqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "73a0fe97-8cd7-4e36-bfc6-3de24f0a26ca"
      },
      "cell_type": "code",
      "source": [
        "num_train_examples = len(X_train)\n",
        "num_val_examples = len(X_test)\n",
        "print(num_train_examples)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2-vAm94xIB9J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f8702869-e85c-4c90-9095-bbd117da272f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import division\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Cropping2D\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "import keras\n",
        "import h5py\n",
        "\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "\n",
        "from keras.optimizers import Nadam\n",
        "from keras.callbacks import History\n",
        "import pandas as pd\n",
        "from keras.backend import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "import random\n",
        "import threading\n",
        "from tensorflow.python.keras import layers\n",
        "from keras.models import model_from_json\n",
        "import tensorflow.contrib as tfcontrib\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import backend as K\n",
        "from keras import utils"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "mw7dBm7qH_7P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_block(input_tensor, num_filters):\n",
        "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same',kernel_initializer='he_uniform')(input_tensor)\n",
        "  encoder = layers.BatchNormalization()(encoder)\n",
        "  encoder = layers.advanced_activations.ELU()(encoder)\n",
        "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same',kernel_initializer='he_uniform')(encoder)\n",
        "  encoder = layers.BatchNormalization()(encoder)\n",
        "  encoder = layers.advanced_activations.ELU()(encoder)\n",
        "  return encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "  encoder = conv_block(input_tensor, num_filters)\n",
        "  encoder_pool = layers.MaxPooling2D((2, 2))(encoder)\n",
        "  \n",
        "  return encoder_pool, encoder\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "  decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same',kernel_initializer='he_uniform')(input_tensor)\n",
        "  #print(input_tensor.shape)\n",
        "  #print(decoder.shape)\n",
        "  decoder = layers.concatenate([concat_tensor, decoder],axis=-1)\n",
        "  #decoder = layers.BatchNormalization()(decoder)\n",
        "  #decoder = layers.Activation('relu')(decoder)\n",
        "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same',kernel_initializer='he_uniform')(decoder)\n",
        "  decoder = layers.BatchNormalization()(decoder)\n",
        "  \n",
        "  \n",
        "  decoder = layers.advanced_activations.ELU()(decoder)\n",
        "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same',kernel_initializer='he_uniform')(decoder)\n",
        "  decoder = layers.BatchNormalization()(decoder)\n",
        "  decoder = layers.advanced_activations.ELU()(decoder)\n",
        "  return decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pPJvYiHBIFKy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=img_shape)\n",
        "\n",
        "# 256\n",
        "#print(inputs.shape)\n",
        "encoder0_pool, encoder0 = encoder_block(inputs, 32)\n",
        "# 128\n",
        "#print(encoder0_pool.shape)\n",
        "encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)\n",
        "# 64\n",
        "\n",
        "encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)\n",
        "# 32\n",
        "\n",
        "encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)\n",
        "# 16\n",
        "\n",
        "encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)\n",
        "# 8\n",
        "\n",
        "center = conv_block(encoder4_pool, 1024)\n",
        "# center\n",
        "\n",
        "decoder4 = decoder_block(center, encoder4, 512)\n",
        "\n",
        "\n",
        "decoder3 = decoder_block(decoder4, encoder3, 256)\n",
        "# 32\n",
        "\n",
        "decoder2 = decoder_block(decoder3, encoder2, 128)\n",
        "# 64\n",
        "\n",
        "decoder1 = decoder_block(decoder2, encoder1, 64)\n",
        "# 128\n",
        "\n",
        "decoder0 = decoder_block(decoder1, encoder0, 32)\n",
        "\n",
        "\n",
        "outputs = layers.Conv2D(3, (1, 1), activation='sigmoid')(decoder0)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iU4k-uZTM2wq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = models.Model(inputs=[inputs], outputs=[outputs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d5AwQsPuMQM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_model_path = 'drive/inter_iit/model/weights_lightgreen.hdf5'\n",
        "cp = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path, monitor='val_dice_loss', save_best_only=True, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aOpw7W_80XN2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Cropping2D\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "import keras\n",
        "import h5py\n",
        "\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "\n",
        "from keras.optimizers import Nadam\n",
        "from keras.callbacks import History\n",
        "import pandas as pd\n",
        "from keras.backend import binary_crossentropy\n",
        "from tensorflow.python.keras import layers\n",
        "\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "import random\n",
        "import threading\n",
        "\n",
        "from keras.models import model_from_json\n",
        "\n",
        "img_rows = 256\n",
        "img_cols = 256\n",
        "\n",
        "\n",
        "smooth = 1e-12\n",
        "\n",
        "num_channels = 3\n",
        "num_mask_channels = 1\n",
        "\n",
        "\n",
        "def jaccard_coef(y_true, y_pred):\n",
        "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
        "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
        "\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "\n",
        "    return K.mean(jac)\n",
        "\n",
        "\n",
        "def jaccard_coef_int(y_true, y_pred):\n",
        "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "\n",
        "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
        "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
        "\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "\n",
        "    return K.mean(jac)\n",
        "\n",
        "\n",
        "def jaccard_coef_loss(y_true, y_pred):\n",
        "    return -K.log(jaccard_coef(y_true, y_pred)) + binary_crossentropy(y_pred, y_true)\n",
        "\n",
        "'''\n",
        "def get_unet0():\n",
        "    inputs = Input(img_shape)\n",
        "    conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(inputs)\n",
        "    conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
        "    conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
        "    conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(conv1)\n",
        "    conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
        "    conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(pool1)\n",
        "    conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
        "    conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
        "    conv2 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(conv2)\n",
        "    conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
        "    conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(pool2)\n",
        "    conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
        "    conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
        "    conv3 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(conv3)\n",
        "    conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
        "    conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(pool3)\n",
        "    conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
        "    conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
        "    conv4 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(conv4)\n",
        "    conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
        "    conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = Convolution2D(512, 3, 3, border_mode='same', init='he_uniform')(pool4)\n",
        "    conv5 = BatchNormalization(mode=0, axis=1)(conv5)\n",
        "    conv5 = keras.layers.advanced_activations.ELU()(conv5)\n",
        "    conv5 = Convolution2D(512, 3, 3, border_mode='same', init='he_uniform')(conv5)\n",
        "    conv5 = BatchNormalization(mode=0, axis=1)(conv5)\n",
        "    conv5 = keras.layers.advanced_activations.ELU()(conv5)\n",
        "  \n",
        "    a=UpSampling2D(size=(2, 2))(conv5)\n",
        "    up6 = layers.concatenate([a, conv4], axis=1)\n",
        "    conv6 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(up6)\n",
        "    conv6 = BatchNormalization(mode=0, axis=1)(conv6)\n",
        "    conv6 = keras.layers.advanced_activations.ELU()(conv6)\n",
        "    conv6 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(conv6)\n",
        "    conv6 = BatchNormalization(mode=0, axis=1)(conv6)\n",
        "    conv6 = keras.layers.advanced_activations.ELU()(conv6)\n",
        "\n",
        "    up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
        "    conv7 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(up7)\n",
        "    conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
        "    conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
        "    conv7 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(conv7)\n",
        "    conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
        "    conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
        "\n",
        "    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
        "    conv8 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(up8)\n",
        "    conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
        "    conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
        "    conv8 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(conv8)\n",
        "    conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
        "    conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
        "\n",
        "    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
        "    conv9 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(up9)\n",
        "    conv9 = BatchNormalization(mode=0, axis=1)(conv9)\n",
        "    conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
        "    conv9 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(conv9)\n",
        "    crop9 = Cropping2D(cropping=((16, 16), (16, 16)))(conv9)\n",
        "    conv9 = BatchNormalization(mode=0, axis=1)(crop9)\n",
        "    conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
        "    conv10 = Convolution2D(num_mask_channels, 1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(input=inputs, output=conv10)\n",
        "\n",
        "    return model\n",
        "  \n",
        "  '''\n",
        "\n",
        "\n",
        "def flip_axis(x, axis):\n",
        "    x = np.asarray(x).swapaxes(axis, 0)\n",
        "    x = x[::-1, ...]\n",
        "    x = x.swapaxes(0, axis)\n",
        "    return x\n",
        "\n",
        "\n",
        "def form_batch(X, y, batch_size):\n",
        "    X_batch = np.zeros((batch_size, img_rows, img_cols ,num_channels))\n",
        "    y_batch = np.zeros((batch_size, num_mask_channels, img_rows, img_cols, num_channels))\n",
        "    X_height = X.shape[1]\n",
        "    X_width = X.shape[2]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        #random_width = random.randint(0, X_width - img_cols - 1)\n",
        "        #random_height = random.randint(0, X_height - img_rows - 1)\n",
        "\n",
        "        random_image = random.randint(0, X.shape[0] - 1)\n",
        "\n",
        "        y_batch[i] = y[random_image]#, :, random_height: random_height + img_rows, random_width: random_width + img_cols]\n",
        "        X_batch[i] = np.array(X[random_image])#, :, random_height: random_height + img_rows, random_width: random_width + img_cols])\n",
        "    return X_batch, y_batch\n",
        "\n",
        "\n",
        "class threadsafe_iter:\n",
        "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
        "    serializing call to the `next` method of given iterator/generator.\n",
        "    \"\"\"\n",
        "    def __init__(self, it):\n",
        "        self.it = it\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        with self.lock:\n",
        "            return self.it.__next__()\n",
        "\n",
        "\n",
        "def threadsafe_generator(f):\n",
        "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
        "    \"\"\"\n",
        "    def g(*a, **kw):\n",
        "        return threadsafe_iter(f(*a, **kw))\n",
        "    return g\n",
        "\n",
        "\n",
        "@threadsafe_generator\n",
        "def batch_generator(X, y, batch_size, horizontal_flip=False, vertical_flip=False, swap_axis=False):\n",
        "    while True:\n",
        "        X_batch, y_batch = form_batch(X, y, batch_size)\n",
        "\n",
        "        for i in range(X_batch.shape[0]):\n",
        "            xb = X_batch[i]\n",
        "            yb = y_batch[i]\n",
        "\n",
        "            if horizontal_flip:\n",
        "                if np.random.random() < 0.5:\n",
        "                    xb = flip_axis(xb, 1)\n",
        "                    yb = flip_axis(yb, 1)\n",
        "\n",
        "            if vertical_flip:\n",
        "                if np.random.random() < 0.5:\n",
        "                    xb = flip_axis(xb, 2)\n",
        "                    yb = flip_axis(yb, 2)\n",
        "\n",
        "            if swap_axis:\n",
        "                if np.random.random() < 0.5:\n",
        "                    xb = xb.swapaxes(1, 2)\n",
        "                    yb = yb.swapaxes(1, 2)\n",
        "\n",
        "            X_batch[i] = xb\n",
        "            y_batch[i] = yb\n",
        "\n",
        "        yield X_batch, y_batch[:, :, 16:16 + img_rows - 32, 16:16 + img_cols - 32]\n",
        "\n",
        "'''\n",
        "def save_model(model, cross):\n",
        "    json_string = model.to_json()\n",
        "    if not os.path.isdir('cache'):\n",
        "        os.mkdir('cache')\n",
        "    json_name = 'architecture_' + cross + '.json'\n",
        "    weight_name = 'model_weights_' + cross + '.h5'\n",
        "    open(os.path.join('cache', json_name), 'w').write(json_string)\n",
        "    model.save_weights(os.path.join('cache', weight_name), overwrite=True)\n",
        "\n",
        "\n",
        "def save_history(history, suffix):\n",
        "    filename = 'history/history_' + suffix + '.csv'\n",
        "    pd.DataFrame(history.history).to_csv(filename, index=False)\n",
        "\n",
        "\n",
        "def read_model(cross=''):\n",
        "    json_name = 'architecture_' + cross + '.json'\n",
        "    weight_name = 'model_weights_' + cross + '.h5'\n",
        "    model = model_from_json(open(os.path.join('../src/cache', json_name)).read())\n",
        "    model.load_weights(os.path.join('../src/cache', weight_name))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "data_path = '../data'\n",
        "now = datetime.datetime.now()\n",
        "\n",
        "print('[{}] Creating and compiling model...'.format(str(datetime.datetime.now())))\n",
        "\n",
        "\n",
        "print('[{}] Reading train...'.format(str(datetime.datetime.now())))\n",
        "f = h5py.File(os.path.join(data_path, 'train_16.h5'), 'r')\n",
        "\n",
        "#X_train = f['train']\n",
        "\n",
        "#y_train = np.array(f['train_mask'])[:, 5]\n",
        "#y_train = np.expand_dims(y_train, 1)\n",
        "print(y_train.shape)\n",
        "\n",
        "train_ids = np.array(f['train_ids'])\n",
        "\n",
        "'''\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        )\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(X_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    #history=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                       \n",
        "\n",
        "batch_size = 2\n",
        "nb_epoch = 50\n",
        "\n",
        "history = History()\n",
        "callbacks = [\n",
        "    history,\n",
        "]\n",
        "#a=batch_generator(X_train, y_train, batch_size, horizontal_flip=True, vertical_flip=True, swap_axis=True)\n",
        "suffix = 'crops_3_'\n",
        "model.compile(optimizer=tf.contrib.opt.NadamOptimizer(learning_rate=1e-3), loss=jaccard_coef_loss, metrics=['binary_crossentropy', jaccard_coef_int])\n",
        "model.fit_generator(datagen.flow(X_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                    \n",
        "                    steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n",
        "                    epochs=nb_epoch,\n",
        "                    verbose=1,\n",
        "                    #samples_per_epoch=batch_size * 400,\n",
        "                    use_multiprocessing=True,\n",
        "                    callbacks=[cp],\n",
        "                     workers=8\n",
        "                    )\n",
        "\n",
        "#save_model(model, \"{batch}_{epoch}_{suffix}\".format(batch=batch_size, epoch=nb_epoch, suffix=suffix))\n",
        "#save_history(history, suffix)\n",
        "#a=batch_generator(X_train, y_train, batch_size, horizontal_flip=True, vertical_flip=True, swap_axis=True)\n",
        "\n",
        "#save_model(model, \"{batch}_{epoch}_{suffix}\".format(batch=batch_size, epoch=nb_epoch, suffix=suffix))\n",
        "#save_history(history, suffix)\n",
        "#f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SjRHApbyc3lg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "suffix = 'crops_4_'\n",
        "model.compile(optimizer=tf.contrib.opt.NadamOptimizer(learning_rate=1e-4), loss=jaccard_coef_loss, metrics=['binary_crossentropy', jaccard_coef_int])\n",
        "model.fit_generator(datagen.flow(X_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "    batch_generator(X_train, y_train, batch_size, horizontal_flip=True, vertical_flip=True, swap_axis=True),\n",
        "    steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n",
        "    epochs=nb_epoch,\n",
        "    verbose=1,\n",
        "    use_multiprocessing=True,\n",
        "    #samples_per_epoch=batch_size * 400,\n",
        "    callbacks=[cp]\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cZiU12tK6K8l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}